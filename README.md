# LÂ³former

Official implementation of the paper:  
**[LÂ³former: Enhanced Multi-Scale Shared Transformer with Local Linear Layer for Long-term Series Forecasting](https://doi.org/10.1016/j.inffus.2025.103398)**


## ğŸ“œ Citation
```bash
@article{xia2025LÂ³former,
  title  = {LÂ³former: Enhanced Multi-Scale Shared Transformer with Local Linear Layer for Long-term Series Forecasting},
  author = {Yulin Xia and Chang Wu and Xiaoman Yang},
  journal= {Information Fusion},
  volume = {103398},
  year   = {2025},
  doi    = {10.1016/j.inffus.2025.103398},
  note   = {Available online: 20 June 2025},
  url    = {https://doi.org/10.1016/j.inffus.2025.103398}
}
```

## ğŸš€ News

-   â€‹**â€‹2025.06â€‹**â€‹ LÂ³former is officially online and release full code of main results.
-   â€‹**â€‹2025.05â€‹**â€‹ LÂ³former is completely accepted by **Information Fusion**.

## ğŸ”Key Features

-   ğŸš€ â€‹**â€‹Local Linear Layer (LÂ³)â€‹**â€‹: Novel neural network layer capturing fine-grained local-temporal patterns
-   ğŸš€ â€‹**â€‹Scale-Wise Attention (SWAM)â€‹**â€‹: Efficient multi-scale feature fusion with O(GÂ²) complexity
-   ğŸš€ â€‹**â€‹Variable-Wise Feed-Forward (VWFF)â€‹**â€‹: Enhanced modeling of inter-variable correlations
-   ğŸš€ â€‹**â€‹Triple-Observation Frameworkâ€‹**â€‹: Simultaneous modeling of scale, temporal, and variable dependencies
-   ğŸš€ â€‹**â€‹Multi-Scale Shared Backboneâ€‹**â€‹: Unified architecture reducing model complexity and accelerating training
-  ğŸš€ â€‹**â€‹State-of-the-art performanceâ€‹**â€‹ in long-term time series forecasting with â€‹**â€‹5.8%-16.7% lower MSEâ€‹**â€‹ averaged by all the datasets compared to previous methods
<p align="center">
  <img src="https://raw.githubusercontent.com/Xia-aaa/L3former/main/figures/Radar.png" width="500"/>
</p>

## ğŸ§  Core Components
### 1ï¸âƒ£ Model Structure (LÂ³former)
**Triple-Observation Frameworkâ€‹**â€‹: Simultaneous modeling of scale, temporal, and variable dependencies

<p align="center">
  <img src="https://raw.githubusercontent.com/Xia-aaa/L3former/main/figures/L3former.png" width="700"/>
</p>

### 2ï¸âƒ£ Local Linear Layer (LÂ³)

Innovative neural layer replacing convolutions/pooling:

â€‹**â€‹Key advantagesâ€‹**â€‹:
-   âœ… Temporal-independent weights for local temporal modeling, and channel-shared weights for cross-variable interaction
-   âœ… Adjustable window sizes for multi-scale feature capture
-   âœ… 49% Faster and 99.9% Fewer Parameters over 1D convolution for high-dimensional data processing


<p align="center">
  <img src="https://raw.githubusercontent.com/Xia-aaa/L3former/main/figures/L3andConv.png" width="700"/>
</p>

</p>

### 3ï¸âƒ£ Scale-Wise Attention (SWAM)

Computationally efficient attention across temporal scales:

â€‹**â€‹Key advantagesâ€‹**â€‹:

-   âœ… Scale number G (â‰¤6) is independent of sequence length and variable numbers, reducing attention computation cost
-   âœ… Effectively fuse cross-scale features

<p align="center">
  <img src="https://raw.githubusercontent.com/Xia-aaa/L3former/main/figures/SWAM.png" width="500"/>
</p>

### 4ï¸âƒ£ Variable-Wise Feed-Forward (VWFF)

Captures complex cross-variable dependencies:

â€‹**â€‹Key advantagesâ€‹**â€‹:

-   âœ… Integrate series-wise embedding to overcome VWFF failure.
-   âœ… Linear complexity with low cost

<p align="center">
  <img src="https://raw.githubusercontent.com/Xia-aaa/L3former/main/figures/VWFF.png" width="700"/>
</p>

##  ğŸ“Š Scability of LÂ³former

-    â€‹**VWFF+PatchTST**â€‹: Table 4 in our paper
-   **â€‹LÂ³+iTransformerâ€‹**â€‹: Table 6 in our paper
-  **â€‹LÂ³+TimeMixer**â€‹: Table 7 in our paper

##  ğŸ“Š Experienments
### 1ï¸âƒ£ Main Results
Table 2 in our paper: 
LÂ³former achieves significant improvements in long-term forecasting across nine benchmark datasets, outperforming state-of-the-art models (in Fig.1) by **5.8%â€“16.7%**  averaged over all datasets in MSE.
- **Large-Scale Datasets**ï¼š On **Solar-Energy**, LÂ³former reduces MSE by **11.1%** compared to the best baseline; on **ECL**, it lowers MSE by **7.9%**. On **Traffic**, while MSE is slightly higher than iTransformer, MAE remains comparable. 
-  **Other Datasets**ï¼šOn **Weather**, LÂ³former improves MSE by **2.6%â€“4.1%**, and on **ETTm** datasets, it surpasses all baselines, including linear-based models like TimeMixer. 
-  **Ultra-long-term forecasting**ï¼šIn 720-step prediction, LÂ³former demonstrates robust performance: it reduces MSE by **13.5% (ECL)**, **11.8% (Solar-Energy)**, and **26.3% (Exchange)**ï¼Œ
### 2ï¸âƒ£ Ablation
Table 8 in our paper
### 3ï¸âƒ£ Local Linear Layer (LÂ³)
âœ… Compared with pooling layer and convolution layer
<p align="center">
  <img src="https://raw.githubusercontent.com/Xia-aaa/L3former/main/figures/L3vsPoolingvsConv.png" width="700"/>
</p>
âœ… Different windows
<p align="center">
  <img src="https://raw.githubusercontent.com/Xia-aaa/L3former/main/figures/Windows.png" width="400"/>
</p>

### 4ï¸âƒ£ VWFF and Temporal shifts in the Traffic dataset
The performance drop of VWFF on the Traffic dataset stems from the **840th anomalous variable** causing model misguidance and temporal drift; removing it restores accuracy, proving VWFFâ€™s robustness in stable-variable scenarios (e.g., ECL, Weather). For Traffic-like data, we suggest to **apply rigorous variable screening or anomaly detection** before deployment.
<p align="center">
  <img src="https://raw.githubusercontent.com/Xia-aaa/L3former/main/figures/Traffic.png" width="600"/>
</p>

### 5ï¸âƒ£ Efficiency
LÂ³former demonstrates significantly lower GPU memory usage and faster computational speed than multi-scale Transformers (e.g., **Pathformer, Crossformer**), even outperforming the linear model **TimeMixer**.
<p align="center">
  <img src="https://raw.githubusercontent.com/Xia-aaa/L3former/main/figures/Efficiency.png" width="700"/>
</p>

### 6ï¸âƒ£ Other experiements
Comprehensive experiments were conducted. For additional details, please refer to the original paper.

## ğŸš‚ Getting Started

### Installation
```bash
cd L3former-main
pip install -r requirements.txt
```

### Data Preparation
The datasets can be obtained from  [Google Drive](https://drive.google.com/file/d/1l51QsKvQPcqILT3DwfjCgx8Dsg2rpjot/view?usp=drive_link)  or  [Baidu Cloud](https://pan.baidu.com/s/11AWXg1Z6UwjHzmto4hesAA?pwd=9qjr).

### Training & Evaluation

Run benchmark experiments:
```bash
# for example: ECL
bash scripts/multivariate_forecasting/ECL.sh
# for example: ETTh1
bash scripts/multivariate_forecasting/ETTh1.sh
```
The you can cheak the results in "./logs". Moreover, we have provided the complete training logs in "./logs/2025".

## ğŸ™ Acknowledgements

We extend gratitude to these foundational works:

-   Informer ([https://github.com/zhouhaoyi/Informer2020](https://github.com/zhouhaoyi/Informer2020))
-   Autoformer ([https://github.com/thuml/Autoformer](https://github.com/thuml/Autoformer))
- PatchTST ([https://github.com/yuqinie98/PatchTST.](https://github.com/yuqinie98/PatchTST))
-   Time-Series-Library ([https://github.com/thuml/Time-Series-Library](https://github.com/thuml/Time-Series-Library))
-   iTransformer ([https://github.com/thuml/iTransformer.](https://github.com/thuml/Time-Series-Library))
- TimeMixer ([https://github.com/kwuking/TimeMixer.](https://github.com/kwuking/TimeMixer))
## ğŸ“§ Contact

Reach our team for collaboration opportunities:

-   Yulin Xia: xyrain1@std.uestc.edu.cn
